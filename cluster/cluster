#!/usr/bin/python3

"""
Load the R Packages that we will need on start up
"""

from sklearn.cluster import k_means

import pandas as pd
import argparse
from numpy import loadtxt, asarray

from rpy2 import robjects
from rpy2.robjects.vectors import FloatVector
from rpy2.robjects.packages import importr, isinstalled
from rpy2.robjects import numpy2ri
from rpy2.robjects.vectors import StrVector
from codecs import open as copen

def readData(filename):

    data = None
    encodings = ('UTF-8', 'UTF-16', 'ascii')
    
    for enc in encodings:
        if data is not None:
            break
        
        with copen(filename, encoding=enc) as f:
            try:
                data = loadtxt(f)
            except UnicodeDecodeError:
                continue

    if data is not None:
        try:
            num_rows, num_cols = data.shape
        except ValueError as e:
            # Indicates that there was a bad number of columns (probably empty)                                                                                                                                 
            # A 1-D array was created, and it must be reshaped for proper use later.                                                                                                                            
            data = data.reshape(-1, 1)

    return data


def readDataFile(filename, delimiter=' ', num_columns=1):
    """                                                                                                                                                                                                     
    Positional Arguments                                                                                                                                                                                    
       : :param filename: Name of the file to read from.                                                                                                                                                    
                                                                                                                                                                                                            
    Optional Arguments                                                                                                                                                                                      
      : :param delimiter: Delimiter that separates the values in the file. The same delimiter                                                                                                               
        should be used in the file.                                                                                                                                                                         
      : :param num_columns: Number of columns to separate the data by. All values in the file                                                                                                               
        are assumed to be floating point numbers.                                                                                                                                                           
                                                                                                                                                                                                            
    :return: N-D Array                                                                                                                                                                                      
    """
    if num_columns > 1:
        nd_arr = loadtxt(filename, delimiter=delimiter, dtype={'formats': (float,int,) * num_columns})
    elif num_columns == 1:
        nd_arr = loadtxt(filename, delimiter=delimiter)
    else:
        raise ValueError

    try:
        num_rows, num_cols = nd_arr.shape
    except ValueError as e:
        # Indicates that there was a bad number of columns (probably empty)                                                                                                                                 
        # A 1-D array was created, and it must be reshaped for proper use later.                                                                                                                            
        nd_arr = nd_arr.reshape(-1, 1)

    return nd_arr


class Metrics:

    def __init__(self):
        """
        
        """
        self.rFunc = None  # callable R function 
        
        self._loadR()

    def _loadR(self):
        """
        Load the R modules and setup the environment for this script
        """
        # R's utility package
        utils = importr('utils')
        utils.chooseCRANmirror(ind=1)
        
        # R package names
        packnames = ('clusterCrit',)
            
        names_to_install = [x for x in packnames if not isinstalled(x)]
        if len(names_to_install) > 0:
            utils.install_packages(StrVector(names_to_install))
    
        # define the R function that this script will use
        robjects.r(
            '''
            # function to utilize the clusterCrit package in R
            # the sklearn module from python does not contain all
            # matrics that we wish to test
            crit <- function(dataset, labels) {    
                # $cluster: Cluster of each observation
                # $centers: Cluster centers
                # $totss: Total sum of squares
                # $withinss: Within sum of square. The number of components return is equal to `k`
                # $tot.withinss: sum of withinss
                # $betweenss: total sum of square minus within sum of square
                # $size: number of observation within each cluster
                ccData <- clusterCrit::intCriteria(dataset, unlist(labels), "all")
                return(list(ccData, clusterCrit::getCriteriaNames(TRUE)))
            }
            '''
        )
        
        # technically this is global but return the `alias` or the callable
        # to the R function so that it can be used later
        self.rFunc = robjects.globalenv['crit']

    @staticmethod
    def prepareData(data_set, k, init):
        """
        :param data_set:
        :param k:
        :param init:
        """
        centroids, matching_clusters, weighted_sum_to_centroids = k_means(data_set, k, init=init)

        # update the array values, in R the indicies started with 1 not 0
        matching_clusters = asarray(matching_clusters) + 1

        # format the data as a list, this is expected by R
        matching_clusters = matching_clusters.tolist()
        return matching_clusters

    def execOnData(self, data_set, labels, k):
        """
        :param data_set:
        :param labels:
        """
        if self.rFunc is not None:
            numpy2ri.activate()
            appliedData, critAlgorithms = self.rFunc(data_set, labels)
            numpy2ri.deactivate()
            return pd.DataFrame(appliedData, index=critAlgorithms, columns=[str(k)])
        
    def prepareAndExec(self, data_set, k, init):
        """
        convenience function. Please see `prepareData` and `execOnData` for 
        more information.
        """
        mc = Metrics.prepareData(data_set, k, init)
        return self.execOnData(data_set, mc, k)


def createArgs():
    """
    Function to stow away the information for arguments
    """
    parser = argparse.ArgumentParser(
        description="Read in files containing data points and execute a set of metrics on the dataset."
    )
    parser.add_argument(
        "--file",
        type=str,
        help="File that will be read in, and a dataset will be made out of the data."
             "The data should NOT include headers for rows or columns."
    )    
    parser.add_argument(
        "-k", "--clusters",
        type=int,
        nargs='+',
        help="List of the number of clusters. The first index in the list is the lower"
             " limit number of clusters, while the second is upper limit."
             " For instance: --clusters 3 10 would run the algorithm on all clusters"
             " in the range of 3 to 10."
             " <1 will be ignored and lists of length 0 are ignored, lists of length 1"
             "indicates that the upper and lower limit are the same."
    )
    parser.add_argument(
        "--init",
        type=str,
        choices=['k-means++', 'random'],
        default='k-means++',
        help="When present, this option inidcates the use of kmeans vs kmeans++ (default)."
    )
    return parser


def createOutputFile(filename):
    """
    :param filename: input file name 
    """
    # find just the filename
    fn = filename.split("/")[-1]
    
    # remove the extension
    fn = fn.split(".")
    if len(fn) > 1:
        fn = fn[0]
        
    # returning the excel spreadsheet name
    return fn + "_analysis.xlsx"

def processClusters(clusters):
    """
    The min cluster numbers should be the first element and the 
    max cluster numbers should be the second element. If only one
    element exists, then the upper and lower limit are the same. 
    
    :param clusters: list of cluster values
    """
    if not isinstance(clusters, list):
        raise AttributeError
    
    # never be less than 0 but you never know
    if len(clusters) <= 0:
        raise AttributeError
    
    if clusters[0] < 1 or clusters[1] < 1:
        raise AttributeError
    
    if len(clusters) == 1:
        return clusters[0], clusters[0]
    else:
        return min(clusters[0], clusters[1]), max(clusters[0], clusters[1])

    
def main():
    """
    main entry point
    """
    parser = createArgs()
    args = parser.parse_args()
    
    m = Metrics()
    
    # graceful fail
    dataSet = readData(args.file)
    
    if dataSet is None:
        print("Error reading data set.")
        exit(1)
        
    try:
        clusterData = processClusters(args.clusters)
    except AttributeError:
        print("ERROR check your cluster values")
        exit(1)

    minCluster, maxCluster = clusterData[0], clusterData[1]

    excelDict = {}
    clusterDict = {}
    # batch these results
    for cluster in range(minCluster, maxCluster+1):
        matching_clusters = Metrics.prepareData(dataSet, cluster, args.init)
        excelDict[cluster] = m.execOnData(dataSet, matching_clusters, cluster)
        clusterDict[cluster] = matching_clusters

    outputFile = createOutputFile(args.file)

    # Write the output to the excel file
    col = 0
    with pd.ExcelWriter(outputFile, engine='xlsxwriter') as w:
        # write all of the metric information to the first sheet
        for sheet, data in excelDict.items():
            data.to_excel(w, sheet_name="metrics", startcol=col, index=col==0)
            col = col + 1 if col > 0 else col + 2
    
        # write all of the cluster information to another sheet
        pd.DataFrame(clusterDict).to_excel(w, sheet_name="clusters")


if __name__ == '__main__':
    main()

